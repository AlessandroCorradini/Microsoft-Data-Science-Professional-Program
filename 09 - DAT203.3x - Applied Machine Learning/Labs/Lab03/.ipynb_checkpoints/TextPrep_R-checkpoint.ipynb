{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications of Data Science\n",
    "# Lab 3\n",
    "# Preparing Text Data with R\n",
    "\n",
    "## Overview\n",
    "\n",
    "This lab introduces you to the basics of text mining and text data preparation. In this lab you will work with a set of 160,000 tweets, which include sentiment labels. \n",
    "\n",
    "Social media sentiment is an important indicator of public opinion.  Determining sentiment can be valuable in a number of applications including brand awareness, product launches, and detecting political trends. \n",
    "\n",
    "Raw text is inherently messy. Machine understanding and analysis is inhibited by the presence of extraneous symbols and words that clutter the text. The exact nature of the required text cleaning depends on the application.  In this case, you will focus on text cleaning to facilitate sentiment classification.The presence of certain words determines the sentiment of the tweet. Words and symbols which are extraneous to this purpose are distractions at best, and a likely source of noise in the analysis. You will follow these steps to prepare the tweet text for analysis: \n",
    "\n",
    "- Remove punctuation symbols and numerals, leaving only alphabetic characters.\n",
    "- Convert all remaining characters to lower case. \n",
    "- Remove stopwords like \"the\", \"and\" and \"this\". Since these words are relatively common, yet communicate no particular sentiment, they can bias analytics. \n",
    "- Stem all remaining words to their root stem. \n",
    "\n",
    "\n",
    "## What you will need\n",
    "To complete this lab, you will need the following:\n",
    "- A web browser and Internet connection\n",
    "- An Azure ML workspace\n",
    "- The lab files for this lab\n",
    "\n",
    "\n",
    "## Load and transform the tweet data\n",
    "\n",
    "As a first step, ensure that you have uploaded the **tweets.csv** and **stopwords.csv ** files as new datasets in your Azure Machine Learning workspace. Then use the following code to load the tweets data set and set the column names to convenient values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tm)\n",
    "library(\"AzureML\")\n",
    "ws <- workspace()\n",
    "dataset <- download.datasets(ws, \"tweets.csv\")\n",
    "colnames(dataset) <- c(\"sentiment\", \"tweets\") # Set the column names\n",
    "head(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the head of the data frame shown above, noticing the content of the two columns.\n",
    "\n",
    "- The **Sentiment** column contains a sentiment score ``{0,4}`` for negative of positive sentiment of the tweet.\n",
    "- The **Tweets** column contains the actual text of the tweet. \n",
    "\n",
    "In order to work with the text in the tweets using the tools in the R tm package, you must convert them to a corpus object. A tm vector corpus is a vector of corpus objects. In this case, the text of each tweet is a single corpus. \n",
    "\n",
    "Execute the code in the cell below to create a tm vector corpus object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.text <- Corpus(VectorSource(dataset['tweets']))\n",
    "class(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Lower Case Symbols with R \n",
    "In this exercise you will take the first steps in preparing the tweet text using custom R code. You will use the R Text Mining package **tm** to perform some basic filtering and cleaning of the tweet text. Using the tools in the tm package you will perform the following steps:\n",
    "\n",
    "- Remove numbers.\n",
    "- Remove punctuation.\n",
    "- Remove excess white space.\n",
    "- Convert to lower case.\n",
    "\n",
    "Each line in the code cell below using the **tm_map** funciton to apply the specified transformation to the text. Execute the code in the cell below to clean and lower case the tweet text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet.text <- tm_map(tweet.text, content_transformer(removeNumbers))\n",
    "tweet.text <- tm_map(tweet.text, content_transformer(removePunctuation))\n",
    "tweet.text <- tm_map(tweet.text, content_transformer(stripWhitespace))\n",
    "tweet.text <- tm_map(tweet.text, content_transformer(tolower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will create a term-document matrix (TDM) of the tweets. A TDM is a sparse matrix structure with the words (terms) in the rows and documents which may or may not contain that word in the columns. The count of word occurrence of the document is contained in the cells.\n",
    "\n",
    "The frequency of words is simply computed by summing the values in the rows. Note that the row_sums function from the slam package is used to deal with the sparse matrix structure. A dataframe is then constructed with the words and their frequencies in descending order. \n",
    "\n",
    "Execute the code in the cell below to compute the TDM, the word frequencies and examine the head of the data frame. This may take a considerable amount of time to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to.WF = function(corpus){\n",
    "    require(tm)\n",
    "    ## Compute a term-document matrix and then \n",
    "    ## compute the word frequencies.\n",
    "    library(slam)\n",
    "    tdm <- TermDocumentMatrix(corpus, control = list(stopwords = FALSE))\n",
    "    tdm <- removeSparseTerms(tdm, 0.9999999)\n",
    "    freq <- row_sums(tdm, na.rm = T)   \n",
    "    ## Sort the word frequency and build a dataframe\n",
    "    ## including the cumulative frequecy of the words.\n",
    "    freq <- sort(freq, decreasing = TRUE)\n",
    "    word.freq <- data.frame(word = factor(names(freq), levels = names(freq)), \n",
    "                        freq = freq)\n",
    "    word.freq['Cum'] <- cumsum(word.freq['freq'])/sum(word.freq$freq)\n",
    "    word.freq\n",
    "}\n",
    "\n",
    "wf = to.WF(tweet.text)\n",
    "head(wf, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the most frequent words are in the head of this data frame. Of these 20 most frequent words, only one, 'good', is likely to convey much information on sentiment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the normalized text in the processed tweets by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data.frame(sentiment = dataset$sentiment, \n",
    "                text = enc2utf8(unlist(sapply(tweet.text, `[`, \"content\"))), stringsAsFactors=F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this text. Notice that all text is lower case and there are no numbers, punctuation or special characters. \n",
    "\n",
    "Now examine the head of the resulting word frequency data frame to determine the following:\n",
    "\n",
    "- What is the percentage of all words for these first 20 words? \n",
    "- Of these 20 words, how many are likely to contibute sentiment information? \n",
    "- Are these 20 words different from the words seen for the raw text? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(wf, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove stop words\n",
    "\n",
    "In the previous section you removed extraneous characters and whitespace from the tweet text. The results show that the most frequent words do not communicate much sentiment information. These frequent words, which are largely extraneous, are known as stop words and should be removed from the text before further analysis. In this exercise you will use custom R code to remove stop words from the tweet text.\n",
    "\n",
    "As a first step you will load the list of stop words, and examine the first 100 by executing the code in the cell below (again, this may take some time to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.words <- download.datasets(ws, \"stopwords.csv\")\n",
    "stop.words = unique(stop.words)\n",
    "stop.words[1:100,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the stop word list and notice the following:\n",
    "\n",
    "- These words are generally common in English language text.\n",
    "- None of these words seem likely to indicate any particular sentiment. \n",
    "- Some of these words, like 'aww', are specialized to this application of analyzing tweets. \n",
    "\n",
    "The code in the cell below applies the **removeWords** operation to the tweet text. Execute the code in the cell to remove the stop words from the tweets and plot the word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.text <- tm_map(tweet.text, removeWords, stop.words[, 'words'])\n",
    "wf = to.WF(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will now examine the head of the resulting word frequency data frame to determine the following:\n",
    "\n",
    "- What is the percentage of all words for these first 20 words? \n",
    "- Of these 20 words, how many are likely to contribute sentiment information? \n",
    "- Are these 20 words different from the words seen for the normalized text? \n",
    "\n",
    "To perform this exercise, apply the **head** function, with the **n = 20** argument, to the **wf** data frame. \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(wf, n = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem the Words\n",
    "\n",
    "You have cleaned the tweet text and removed stop words. There is one last data preparation step required, stemming the words. Stemming is a process of reducing words to their stems or roots. For example, conjugated verbs such as \"goes\", \"going\", and \"gone\" are stemmed to the word \"go\".  Both Python and R offer a choice of stemmers. Depending on this choice, the results can be more or less suitable for the application. In this case, you will use the popular Porter stemmer. \n",
    "\n",
    "The Porter stemmer is in the R **SnowBallC** library. Execute the code in the cell below to load and apply the Porter stemmer to the tweet text (again, this may take some time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(SnowballC) ## For stemming words\n",
    "tweet.text <- tm_map(tweet.text, stemDocument)\n",
    "wf = to.WF(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, examine a sample of the prepared tweet text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data.frame(sentiment = dataset$sentiment, \n",
    "                text = enc2utf8(unlist(sapply(tweet.text, `[`, \"content\"))), stringsAsFactors=F))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You will now examine the head of the resulting word frequency data frame to determine the following:\n",
    "\n",
    "- Have any of the words in the list been stemmed? \n",
    "- Has the stemming changed the frequency of these words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(wf, n=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
